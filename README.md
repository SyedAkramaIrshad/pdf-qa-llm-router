# PDF QA System with LLM-Based Hierarchical Routing

An intelligent PDF Question Answering system that uses **LLM-based semantic routing** instead of traditional vector similarity search for precise document navigation.

Scope: Current POC is single-PDF and does not use a vector DB.

## ğŸ¯ Key Innovation

**Current POC (What's Built):**
1. **Ingestion**: Chunk PDF into 10-page sections â†’ Summarize with LLM â†’ Store JSON summaries
2. **Query**: LLM reads JSON summaries â†’ Predicts relevant pages with reasoning
3. **Answer**: Fetch full pages â†’ LLM generates answer with citations

**Why This Works Better Than Traditional RAG:**
- âœ… **No vector database required** - Works with any PDF immediately
- âœ… **Larger context** - 10-page sections preserve content relationships
- âœ… **Smarter routing** - LLM understands semantics vs keyword matching
- âœ… **Explainable** - See LLM's reasoning behind section/page selection
- âœ… **Self-correcting** - Tool errors feed back to LLM for re-routing
- âœ… **Vision support** - Extract and analyze PDF images

## ğŸš€ Features

- **GLM-4.5/4.6V Flash** - Free models (no billing required)
- **10-page chunking** with sequential LLM summarization
- **LLM-based routing** with explainable reasoning
- **Error correction** - Self-correcting page predictions
- **Vision support** - Extract and analyze PDF images
- **CLI interface** - Simple commands for Q&A
- **Configurable concurrency** - Control parallelism vs rate limits

## ğŸ“‹ Requirements

```
# Core
langchain-core>=0.2.0
langgraph>=0.2.0
httpx>=0.28.0
pydantic>=2.0.0
pydantic-settings>=2.0.0
tenacity>=9.0.0

# PDF Processing
pypdf>=3.0.0
pdfplumber>=0.11.0
pdf2image>=1.17.0
Pillow>=11.0.0

# CLI
click>=8.0.0
```

## âš™ï¸ Configuration

```bash
# .env file
GLM_API_KEY=your_api_key_here
GLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4/
GLM_MODEL=glm-4.5-flash
GLM_VISION_MODEL=glm-4.6v-flash

CHUNK_SIZE=10              # Pages per section
INDEXING_CONCURRENT=1      # 1=sequential, >1=parallel
API_DELAY=1.0              # Delay between API calls
MAX_CONCURRENT_CALLS=3     # Max parallel LLM calls
```

## ğŸ“– Usage

```bash
# Install dependencies
pip install -r requirements.txt

# Configure API key
cp .env.example .env
# Edit .env with your GLM API key

# Ask a question
python pdfqa.py ask path/to/document.pdf "What is this about?"

# Interactive mode
python pdfqa.py ask path/to/document.pdf -i

# Index a PDF
python pdfqa.py index path/to/document.pdf

# Show configuration
python pdfqa.py config
```

## ğŸ—ï¸ Current Architecture (POC)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PDF INGESTION PIPELINE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load single PDF file                                    â”‚
â”‚  2. Chunk into 10-page sections                             â”‚
â”‚  3. Summarize each section with LLM â†’ JSON summaries        â”‚
â”‚  4. Store summaries in memory (for single PDF)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QUERY PIPELINE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. User asks question                                      â”‚
â”‚  2. LLM reads JSON summaries â†’ predicts relevant pages      â”‚
â”‚     (with explainable reasoning)                            â”‚
â”‚  3. Fetch full page text + images                           â”‚
â”‚  4. LLM generates answer with citations                     â”‚
â”‚  5. Error correction: Tool failures â†’ LLM re-routes         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Future Extension: Hybrid Retrieval (Not Implemented)

Future extension (not implemented in this POC): vector-DB-based document discovery
for multi-PDF scale.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 HYBRID RETRIEVAL ARCHITECTURE               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ğŸ“ DOCUMENT DISCOVERY (Coarse Level)                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  1. Embed chunks from all PDFs â†’ Vector DB                 â”‚
â”‚  2. Query â†’ Top 50 chunks (with filename metadata)          â”‚
â”‚  3. Aggregate by filename â†’ Top N unique PDFs               â”‚
â”‚     (e.g., "Which 5 documents are most relevant?")          â”‚
â”‚                                                             â”‚
â”‚  ğŸ¯ PRECISE NAVIGATION (Fine Level)                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  4. Load pre-computed JSON summaries for Top N PDFs        â”‚
â”‚  5. Parallel LLM routing (1 call per PDF) â†’ Predict pages   â”‚
â”‚     (Cost scales with N unique files, not fixed)            â”‚
â”‚                                                             â”‚
â”‚  ğŸ’¡ ANSWER GENERATION                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  6. Fetch pages â†’ LLM generates answer with citations       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Why Hybrid?**
- Vector DB is great for document discovery (1000+ PDFs)
- LLM routing is better for precise page navigation within documents
- Combines speed of vector search + semantic understanding of LLM
```

## ğŸ“‚ Project Structure

```
10_feb/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/          # LangGraph workflow
â”‚   â”‚   â””â”€â”€ graph.py    # Router, Fetcher, Answer Generator
â”‚   â”œâ”€â”€ cli/            # Command-line interface
â”‚   â”‚   â””â”€â”€ main.py     # Click commands
â”‚   â”œâ”€â”€ config/         # Pydantic settings
â”‚   â”‚   â””â”€â”€ settings.py  # Configuration with validation
â”‚   â”œâ”€â”€ llm/            # GLM API client
â”‚   â”‚   â”œâ”€â”€ client.py    # Text + Vision API calls
â”‚   â”‚   â””â”€â”€ prompts.py   # Prompt templates
â”‚   â”œâ”€â”€ pdf/            # PDF processor
â”‚   â”‚   â””â”€â”€ processor.py # Text + image extraction
â”‚   â””â”€â”€ storage/        # Vector DB and metadata storage (for scaling)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/           # PDF files (not in git)
â”‚   â”œâ”€â”€ indices/        # Vector DB indices
â”‚   â””â”€â”€ summaries/      # Pre-computed JSON summaries
â”œâ”€â”€ pdfqa.py           # Main entry point
â”œâ”€â”€ requirements.txt   # Dependencies
â””â”€â”€ .env.example       # Configuration template
```

## ğŸš€ Extending to Scale

This concept can be extended to handle 1000+ PDFs using a hybrid approach:

**Document Discovery (Vector DB):**
- Embed all PDF chunks into a vector database
- Query returns top relevant chunks with filename metadata
- Aggregate by filename to identify Top N most relevant PDFs

**Precise Navigation (LLM Routing):**
- Load pre-computed JSON summaries for the Top N PDFs
- Run parallel LLM routing (one call per PDF) to predict pages
- Fetch pages and generate answer with citations

**Why Hybrid?**
- Vector DB excels at finding relevant documents at scale
- LLM routing provides semantic understanding within documents
- Cost scales with O(N) where N = unique files in results, not total documents

## ğŸ” Security & Privacy

- **No data in repository** - All PDFs and API keys excluded
- **Environment-based config** - Sensitive data in `.env` (gitignored)
- **Rate limiting** - Configurable delays and concurrency
- **Input validation** - Pydantic validates all settings

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For issues and questions, please open an issue on GitHub.
